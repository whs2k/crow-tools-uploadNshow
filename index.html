<!DOCTYPE html>
<html lang="en" class="">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Crow Call Audio Classifier</title>
    <!-- Tailwind CSS for styling -->
    <script src="https://cdn.tailwindcss.com"></script>
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;700&display=swap" rel="stylesheet">
    <!-- ONNX Runtime for web-based ML inference -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
    <!-- Highlight.js for JSON output styling -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

    <style>
        body {
            font-family: 'Inter', sans-serif;
        }
        .action-button {
            transition: all 0.2s ease-in-out;
        }
        .action-button:not(:disabled):hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
        }
        .action-button:disabled {
            cursor: not-allowed;
            transform: none;
            box-shadow: none;
        }
        pre code.hljs {
            border-radius: 0.5rem; /* rounded-lg */
        }
        /* Dark Mode Toggle Styles */
        .toggle-checkbox:checked {
            right: 0;
            border-color: #4F46E5; /* indigo-600 */
        }
        .toggle-checkbox:checked + .toggle-label {
            background-color: #4F46E5; /* indigo-600 */
        }
    </style>
    <script>
        // Set up Tailwind dark mode
        tailwind.config = {
            darkMode: 'class',
        }
    </script>
</head>
<body class="bg-white dark:bg-gray-900 text-gray-800 dark:text-gray-200 transition-colors duration-300">

    <!-- Dark Mode Toggle -->
    <div class="absolute top-4 left-4">
        <div class="flex items-center">
            <span class="mr-2 text-sm font-medium text-gray-900 dark:text-gray-300">‚òÄÔ∏è</span>
            <div class="relative">
                <input type="checkbox" id="dark-toggle" class="sr-only">
                <div class="block bg-gray-600 w-14 h-8 rounded-full"></div>
                <div class="dot absolute left-1 top-1 bg-white w-6 h-6 rounded-full transition"></div>
            </div>
            <span class="ml-2 text-sm font-medium text-gray-900 dark:text-gray-300">üåô</span>
        </div>
    </div>
    
    <!-- Main Content -->
    <div class="flex items-center justify-center min-h-screen p-4">
        <div class="w-full max-w-3xl mx-auto">
            <div class="bg-gray-50 dark:bg-gray-800 rounded-xl shadow-lg p-8 border border-gray-200 dark:border-gray-700">
                
                <header class="text-center mb-8">
                    <h1 class="text-3xl sm:text-4xl font-bold text-gray-900 dark:text-white mb-2">Crow Call Classifier</h1>
                    <p class="text-gray-600 dark:text-gray-400">Upload your model and audio to run inference in the browser.</p>
                </header>

                <div class="space-y-6">
                    <!-- Step 1: Model Upload -->
                    <div>
                        <h2 class="text-lg font-semibold mb-2"><span class="bg-gray-800 dark:bg-gray-600 text-white rounded-full w-8 h-8 inline-flex items-center justify-center mr-2">1</span> Upload ONNX Model</h2>
                        <input type="file" id="modelUploader" accept=".onnx" class="hidden">
                        <label for="modelUploader" class="action-button inline-block bg-gray-800 dark:bg-gray-200 text-white dark:text-gray-800 font-semibold py-3 px-6 rounded-lg cursor-pointer">
                            Select Model (.onnx)
                        </label>
                        <p id="modelFileName" class="text-sm text-gray-500 dark:text-gray-400 mt-2 ml-1">No model selected.</p>
                    </div>

                    <!-- Step 2: Audio File Upload -->
                    <div>
                        <h2 class="text-lg font-semibold mb-2"><span class="bg-gray-800 dark:bg-gray-600 text-white rounded-full w-8 h-8 inline-flex items-center justify-center mr-2">2</span> Choose Audio File</h2>
                        <div class="flex items-center space-x-4">
                             <label for="audioUploader" id="audioUploadLabel" class="action-button inline-block bg-gray-800 dark:bg-gray-200 text-white dark:text-gray-800 font-semibold py-3 px-6 rounded-lg cursor-pointer opacity-50" disabled>
                                Select Audio
                            </label>
                            <input type="file" id="audioUploader" accept="audio/*" class="hidden" disabled>
                            <button id="playbackBtn" class="action-button p-3 rounded-full bg-gray-200 dark:bg-gray-700 text-gray-800 dark:text-gray-200 disabled:bg-gray-100 dark:disabled:bg-gray-800 disabled:text-gray-400" disabled>
                                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor"><path d="M8 5v14l11-7z"></path></svg>
                            </button>
                            <p id="audioFileName" class="text-sm text-gray-500 dark:text-gray-400">No file selected.</p>
                        </div>
                    </div>

                    <!-- Step 3: Classification -->
                    <div>
                         <h2 class="text-lg font-semibold mb-2"><span class="bg-gray-800 dark:bg-gray-600 text-white rounded-full w-8 h-8 inline-flex items-center justify-center mr-2">3</span> Run Analysis</h2>
                        <button id="classifyBtn" class="action-button w-full bg-gray-900 dark:bg-white text-white dark:text-black font-bold py-3 px-6 rounded-lg disabled:bg-gray-400 dark:disabled:bg-gray-600" disabled>
                            Classify Crow Call
                        </button>
                    </div>

                    <!-- Step 4: Output -->
                    <div>
                        <h2 class="text-lg font-semibold mb-2"><span class="bg-gray-800 dark:bg-gray-600 text-white rounded-full w-8 h-8 inline-flex items-center justify-center mr-2">4</span> Results</h2>
                        <div id="outputContainer" class="mt-4 w-full min-h-[200px] bg-gray-900 rounded-lg flex items-center justify-center p-4 transition-all duration-300">
                            <p class="text-gray-400">Upload a model to begin.</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <audio id="audioPlayer" class="hidden"></audio>

    <script>
        // --- Element References ---
        const modelUploader = document.getElementById('modelUploader');
        const modelFileNameDisplay = document.getElementById('modelFileName');
        const audioUploader = document.getElementById('audioUploader');
        const audioUploadLabel = document.getElementById('audioUploadLabel');
        const audioFileNameDisplay = document.getElementById('audioFileName');
        const classifyBtn = document.getElementById('classifyBtn');
        const playbackBtn = document.getElementById('playbackBtn');
        const audioPlayer = document.getElementById('audioPlayer');
        const outputContainer = document.getElementById('outputContainer');
        const darkToggle = document.getElementById('dark-toggle');
        const toggleDot = darkToggle.nextElementSibling.nextElementSibling;
        
        // --- State Management ---
        let selectedAudioFile = null;
        let onnxSession = null;

        /**
         * Preprocesses the audio file into a tensor for the model.
         * !!! CRITICAL !!! This function MUST be modified to perfectly match
         * the preprocessing used to train your Python model (e.g., sample rate,
         * FFT window size, hop length, number of mel bins).
         * @param {File} audioFile The audio file to process.
         * @returns {Promise<ort.Tensor>} A tensor ready for the ONNX model.
         */
        async function preprocessAudio(audioFile) {
            // This is a placeholder for real audio preprocessing.
            const height = 128; 
            
            // FIX: The error "Got invalid dimensions... index: 1 Got: 87 Expected: 768"
            // means the model's second dimension expects a size of 768.
            const width = 768;

            const dummySpectrogram = new Float32Array(height * width);
            for (let i = 0; i < dummySpectrogram.length; i++) {
                dummySpectrogram[i] = Math.random();
            }
            
            const tensor = new ort.Tensor('float32', dummySpectrogram, [height, width]);
            
            console.log("Created dummy input tensor with shape [128, 768]:", tensor);
            return tensor;
        }

        /**
         * Postprocesses the model's raw output logits into the final JSON format.
         * !!! CRITICAL !!! This function MUST be modified to match your model's
         * specific output structure.
         * @param {ort.Tensor} outputTensor The raw output from the model.
         * @returns {object} The final, formatted JSON object.
         */
        function postprocessOutput(outputTensor) {
            const data = outputTensor.data;
            console.log("Raw model output logits:", data);
            const a = (val) => val > 0.5;
            return {
                "start_time": 37.0,
                "end_time": 38.0,
                "crowCount": Math.floor(data[0]) + 1,
                "crowAge": data[1] > 0 ? 1 : 0,
                "alert": a(data[2]), "begging": a(data[3]), "softSong": a(data[4]),
                "rattle": a(data[5]), "mob": a(data[6]),
                "quality": Math.min(Math.max(Math.round(data[7]), 1), 3)
            };
        }

        /**
         * Displays the output JSON in a formatted, highlighted code block.
         */
        function displayOutput(data) {
            const jsonString = JSON.stringify(data, null, 2);
            const pre = document.createElement('pre');
            const code = document.createElement('code');
            code.className = 'language-json';
            code.textContent = jsonString;
            hljs.highlightElement(code);
            outputContainer.innerHTML = '';
            outputContainer.appendChild(pre);
        }

        // --- Event Listeners ---
        modelUploader.addEventListener('change', async function(event) {
            const file = event.target.files[0];
            if (!file) return;

            modelFileNameDisplay.textContent = `Loading: ${file.name}`;
            outputContainer.innerHTML = '<p class="text-gray-400 animate-pulse">Loading ML Model...</p>';
            
            try {
                const buffer = await file.arrayBuffer();
                onnxSession = await ort.InferenceSession.create(buffer);
                
                modelFileNameDisplay.textContent = `Loaded: ${file.name}`;
                outputContainer.innerHTML = '<p class="text-gray-400">Model loaded. Please select an audio file.</p>';
                
                // Enable audio upload
                audioUploader.disabled = false;
                audioUploadLabel.classList.remove('opacity-50');
                audioUploadLabel.removeAttribute('disabled');

            } catch (error) {
                console.error("Failed to load the ONNX model:", error);
                outputContainer.innerHTML = `<p class="text-red-400">Error: Could not load model. Check console for details.</p>`;
                modelFileNameDisplay.textContent = `Failed to load: ${file.name}`;
                onnxSession = null;
                 // Disable audio upload
                audioUploader.disabled = true;
                audioUploadLabel.classList.add('opacity-50');
                audioUploadLabel.setAttribute('disabled', 'true');
            }
             // Always disable classify button when model changes
            classifyBtn.disabled = true;
        });

        audioUploader.addEventListener('change', function(event) {
            if (event.target.files && event.target.files[0]) {
                selectedAudioFile = event.target.files[0];
                audioFileNameDisplay.textContent = `Selected: ${selectedAudioFile.name}`;
                classifyBtn.disabled = !onnxSession;
                
                const fileURL = URL.createObjectURL(selectedAudioFile);
                audioPlayer.src = fileURL;
                playbackBtn.disabled = false;
            } else {
                selectedAudioFile = null;
                audioFileNameDisplay.textContent = 'No file selected.';
                classifyBtn.disabled = true;
                playbackBtn.disabled = true;
            }
        });

        classifyBtn.addEventListener('click', async function() {
            if (!selectedAudioFile || !onnxSession) return;

            outputContainer.innerHTML = '<p class="text-gray-400 animate-pulse">Analyzing audio...</p>';
            classifyBtn.disabled = true;
            classifyBtn.textContent = 'Processing...';

            try {
                const inputTensor = await preprocessAudio(selectedAudioFile);

                const inputName = onnxSession.inputNames[0];
                const outputName = onnxSession.outputNames[0];

                const feeds = { [inputName]: inputTensor };
                
                const results = await onnxSession.run(feeds);
                const outputTensor = results[outputName];

                const finalOutput = postprocessOutput(outputTensor);
                displayOutput(finalOutput);

            } catch (error) {
                console.error("Error during classification:", error);
                outputContainer.innerHTML = `<p class="text-red-400">Error during classification. Check console.</p>`;
            } finally {
                classifyBtn.disabled = false;
                classifyBtn.textContent = 'Classify Crow Call';
            }
        });
        
        playbackBtn.addEventListener('click', () => audioPlayer.play());

        darkToggle.addEventListener('change', () => {
            if (darkToggle.checked) {
                document.documentElement.classList.add('dark');
                toggleDot.style.transform = 'translateX(100%)';
            } else {
                document.documentElement.classList.remove('dark');
                toggleDot.style.transform = 'translateX(0)';
            }
        });
    </script>
</body>
</html>
