<!DOCTYPE html>
<html lang="en" class="">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Crow Call Audio Classifier</title>
    <!-- Tailwind CSS for styling -->
    <script src="https://cdn.tailwindcss.com"></script>
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;700&display=swap" rel="stylesheet">
    <!-- ONNX Runtime for web-based ML inference -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
    <!-- Highlight.js for JSON output styling -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

    <style>
        body {
            font-family: 'Inter', sans-serif;
        }
        .action-button {
            transition: all 0.2s ease-in-out;
        }
        .action-button:not(:disabled):hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
        }
        .action-button:disabled {
            cursor: not-allowed;
            transform: none;
            box-shadow: none;
        }
        pre code.hljs {
            border-radius: 0.5rem; /* rounded-lg */
        }
        /* Dark Mode Toggle Styles */
        .toggle-checkbox:checked {
            right: 0;
            border-color: #4F46E5; /* indigo-600 */
        }
        .toggle-checkbox:checked + .toggle-label {
            background-color: #4F46E5; /* indigo-600 */
        }
    </style>
    <script>
        // Set up Tailwind dark mode
        tailwind.config = {
            darkMode: 'class',
        }
    </script>
</head>
<body class="bg-white dark:bg-gray-900 text-gray-800 dark:text-gray-200 transition-colors duration-300">

    <!-- Dark Mode Toggle -->
    <div class="absolute top-4 left-4">
        <div class="flex items-center">
            <span class="mr-2 text-sm font-medium text-gray-900 dark:text-gray-300">‚òÄÔ∏è</span>
            <div class="relative">
                <input type="checkbox" id="dark-toggle" class="sr-only">
                <div class="block bg-gray-600 w-14 h-8 rounded-full"></div>
                <div class="dot absolute left-1 top-1 bg-white w-6 h-6 rounded-full transition"></div>
            </div>
            <span class="ml-2 text-sm font-medium text-gray-900 dark:text-gray-300">üåô</span>
        </div>
    </div>
    
    <!-- Main Content -->
    <div class="flex items-center justify-center min-h-screen p-4">
        <div class="w-full max-w-3xl mx-auto">
            <div class="bg-gray-50 dark:bg-gray-800 rounded-xl shadow-lg p-8 border border-gray-200 dark:border-gray-700">
                
                <header class="text-center mb-8">
                    <h1 class="text-3xl sm:text-4xl font-bold text-gray-900 dark:text-white mb-2">Crow Call Classifier</h1>
                    <p class="text-gray-600 dark:text-gray-400">Upload audio to run a real ML model in your browser.</p>
                </header>

                <div class="space-y-6">
                    <!-- Step 1: File Upload -->
                    <div>
                        <h2 class="text-lg font-semibold mb-2"><span class="bg-gray-800 dark:bg-gray-600 text-white rounded-full w-8 h-8 inline-flex items-center justify-center mr-2">1</span> Choose Audio File</h2>
                        <div class="flex items-center space-x-4">
                            <input type="file" id="audioUploader" accept="audio/*" class="hidden">
                            <label for="audioUploader" class="action-button inline-block bg-gray-800 dark:bg-gray-200 text-white dark:text-gray-800 font-semibold py-3 px-6 rounded-lg cursor-pointer">
                                Select Audio
                            </label>
                            <button id="playbackBtn" class="action-button p-3 rounded-full bg-gray-200 dark:bg-gray-700 text-gray-800 dark:text-gray-200 disabled:bg-gray-100 dark:disabled:bg-gray-800 disabled:text-gray-400" disabled>
                                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor"><path d="M8 5v14l11-7z"></path></svg>
                            </button>
                            <p id="fileName" class="text-sm text-gray-500 dark:text-gray-400">No file selected.</p>
                        </div>
                    </div>

                    <!-- Step 2: Classification -->
                    <div>
                         <h2 class="text-lg font-semibold mb-2"><span class="bg-gray-800 dark:bg-gray-600 text-white rounded-full w-8 h-8 inline-flex items-center justify-center mr-2">2</span> Run Analysis</h2>
                        <button id="classifyBtn" class="action-button w-full bg-gray-900 dark:bg-white text-white dark:text-black font-bold py-3 px-6 rounded-lg disabled:bg-gray-400 dark:disabled:bg-gray-600" disabled>
                            Classify Crow Call
                        </button>
                    </div>

                    <!-- Step 3: Output -->
                    <div>
                        <h2 class="text-lg font-semibold mb-2"><span class="bg-gray-800 dark:bg-gray-600 text-white rounded-full w-8 h-8 inline-flex items-center justify-center mr-2">3</span> Results</h2>
                        <div id="outputContainer" class="mt-4 w-full min-h-[200px] bg-gray-900 rounded-lg flex items-center justify-center p-4 transition-all duration-300">
                            <p class="text-gray-400">Classification output will appear here...</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <audio id="audioPlayer" class="hidden"></audio>

    <script>
        // --- Element References ---
        const audioUploader = document.getElementById('audioUploader');
        const fileNameDisplay = document.getElementById('fileName');
        const classifyBtn = document.getElementById('classifyBtn');
        const playbackBtn = document.getElementById('playbackBtn');
        const audioPlayer = document.getElementById('audioPlayer');
        const outputContainer = document.getElementById('outputContainer');
        const darkToggle = document.getElementById('dark-toggle');
        const toggleDot = darkToggle.nextElementSibling.nextElementSibling;
        
        // --- State Management ---
        let selectedFile = null;
        let onnxSession = null;

        // --- Core Functions ---
        
        /**
         * Initialize the ONNX runtime and load the model.
         */
        async function initializeModel() {
            try {
                // Update the output to show model is loading
                outputContainer.innerHTML = '<p class="text-gray-400 animate-pulse">Loading ML Model...</p>';
                // IMPORTANT: Place your converted "crow_model.onnx" file in the same directory as this HTML file.
                onnxSession = await ort.InferenceSession.create('./crow_model.onnx');
                outputContainer.innerHTML = '<p class="text-gray-400">Model loaded. Please select an audio file.</p>';
            } catch (error) {
                console.error("Failed to load the ONNX model:", error);
                outputContainer.innerHTML = `<p class="text-red-400">Error: Could not load model. Make sure 'crow_model.onnx' is in the same folder.</p>`;
            }
        }

        /**
         * Preprocesses the audio file into a tensor for the model.
         * !!! CRITICAL !!! This function MUST be modified to perfectly match
         * the preprocessing used to train your Python model (e.g., sample rate,
         * FFT window size, hop length, number of mel bins).
         * @param {File} audioFile The audio file to process.
         * @returns {Promise<ort.Tensor>} A tensor ready for the ONNX model.
         */
        async function preprocessAudio(audioFile) {
            // This is a placeholder for real audio preprocessing.
            // A real implementation would use the Web Audio API to decode the audio,
            // resample it, and then calculate the spectrogram (e.g., using a JS DSP library).
            
            // For this demo, we'll create a dummy tensor of the correct shape and type.
            // Shape: [batch_size, channels, height, width] -> [1, 1, 128, 87]
            const height = 128; // Corresponds to n_mels
            const width = 87;   // Corresponds to time frames
            const dummySpectrogram = new Float32Array(height * width);
            for (let i = 0; i < dummySpectrogram.length; i++) {
                dummySpectrogram[i] = Math.random(); // Fill with random data
            }
            
            const tensor = new ort.Tensor('float32', dummySpectrogram, [1, 1, height, width]);
            console.log("Created dummy input tensor:", tensor);
            return tensor;
        }

        /**
         * Postprocesses the model's raw output logits into the final JSON format.
         * !!! CRITICAL !!! This function MUST be modified to match your model's
         * specific output structure.
         * @param {ort.Tensor} outputTensor The raw output from the model.
         * @returns {object} The final, formatted JSON object.
         */
        function postprocessOutput(outputTensor) {
            const data = outputTensor.data; // This is a Float32Array of logits
            console.log("Raw model output logits:", data);
            
            // This is a placeholder for converting logits to meaningful output.
            // You would apply sigmoid, argmax, etc., based on your model's design.
            const a = (val) => val > 0.5; // Example: sigmoid thresholding for boolean flags

            return {
                "start_time": 37.0, // This would be derived from analysis
                "end_time": 38.0,   // This would be derived from analysis
                "crowCount": Math.floor(data[0]) + 1, // Example: mapping a logit to count
                "crowAge": data[1] > 0 ? 1 : 0, // Example: mapping a logit to age
                "alert": a(data[2]),
                "begging": a(data[3]),
                "softSong": a(data[4]),
                "rattle": a(data[5]),
                "mob": a(data[6]),
                "quality": Math.min(Math.max(Math.round(data[7]), 1), 3) // Example
            };
        }

        /**
         * Displays the output JSON in a formatted, highlighted code block.
         */
        function displayOutput(data) {
            const jsonString = JSON.stringify(data, null, 2);
            const pre = document.createElement('pre');
            const code = document.createElement('code');
            code.className = 'language-json';
            code.textContent = jsonString;
            hljs.highlightElement(code);
            outputContainer.innerHTML = '';
            outputContainer.appendChild(pre);
        }

        // --- Event Listeners ---
        audioUploader.addEventListener('change', function(event) {
            if (event.target.files && event.target.files[0]) {
                selectedFile = event.target.files[0];
                fileNameDisplay.textContent = `Selected: ${selectedFile.name}`;
                classifyBtn.disabled = !onnxSession; // Enable only if model is loaded
                
                // Set up audio playback
                const fileURL = URL.createObjectURL(selectedFile);
                audioPlayer.src = fileURL;
                playbackBtn.disabled = false;
            } else {
                selectedFile = null;
                fileNameDisplay.textContent = 'No file selected.';
                classifyBtn.disabled = true;
                playbackBtn.disabled = true;
            }
        });

        classifyBtn.addEventListener('click', async function() {
            if (!selectedFile || !onnxSession) return;

            outputContainer.innerHTML = '<p class="text-gray-400 animate-pulse">Analyzing audio...</p>';
            classifyBtn.disabled = true;
            classifyBtn.textContent = 'Processing...';

            try {
                // 1. Preprocess the audio file into a tensor
                const inputTensor = await preprocessAudio(selectedFile);
                
                // 2. Prepare the inputs for the model
                const feeds = { 'input_spectrogram': inputTensor }; // Use the input name from your ONNX model

                // 3. Run inference
                const results = await onnxSession.run(feeds);
                const outputTensor = results['output_logits']; // Use the output name from your ONNX model

                // 4. Postprocess the output
                const finalOutput = postprocessOutput(outputTensor);

                // 5. Display the final result
                displayOutput(finalOutput);

            } catch (error) {
                console.error("Error during classification:", error);
                outputContainer.innerHTML = `<p class="text-red-400">Error during classification. Check console for details.</p>`;
            } finally {
                classifyBtn.disabled = false;
                classifyBtn.textContent = 'Classify Crow Call';
            }
        });
        
        playbackBtn.addEventListener('click', () => audioPlayer.play());

        darkToggle.addEventListener('change', () => {
            if (darkToggle.checked) {
                document.documentElement.classList.add('dark');
                toggleDot.style.transform = 'translateX(100%)';
            } else {
                document.documentElement.classList.remove('dark');
                toggleDot.style.transform = 'translateX(0)';
            }
        });

        // --- Initialization ---
        initializeModel();

    </script>
</body>
</html>
